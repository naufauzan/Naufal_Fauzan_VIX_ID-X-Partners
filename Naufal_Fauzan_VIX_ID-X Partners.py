# -*- coding: utf-8 -*-
"""vix

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KyWpK8VSkkRBV80RLaUPusjPpCn5xSde

#**Credit Risk Assessment**
* Created by Naufal Fauzan
* Final project of VIX in ID/X Partners

#**Business Understanding**
The task is to build a model that can predict credit risk using a dataset provided by the company consisting of accepted and rejected loan data. Besides, visual media needed to be prepared to present the solution to the client.
#**Data Collection**

Dataset collected by ID/X Partners from a company
"""

#Mount Drive
from google.colab import drive
drive.mount('/content/drive/')

"""# IMPORT LIBRARIES AND DATA

---
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

import seaborn as sns
sns.set()

import math
from google.colab import autoviz


import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore")
from sklearn.preprocessing import OneHotEncoder

df_create = pd.read_csv('/content/drive/MyDrive/PIB/loan_data_2007_2014.csv')
df_create.head()

"""# EXPLORING DATA

---
"""

df_create.shape

df_create.info()

df_create.sample()

df_create.id.nunique()

df_create.member_id.nunique()

"""- This dataset contains 466,285 rows and has 75 columns.
The data types are float64(46), int64(7), object(22).
- There are no duplicate `id` or `member_id`, meaning that each row already represents one individual.
- Next, remove features that are unnecessary. Features such as unique id, free text, NULL values, etc. are removed.
"""

cols_to_drop = [
    # the unique id
    'id'
    , 'member_id'

    # free text
    , 'url'
    , 'desc'

    # Some columns contain null values and columns that are not useful for modeling.
    , 'Unnamed: 0'
    , 'zip_code'
    , 'annual_inc_joint'
    , 'dti_joint'
    , 'verification_status_joint'
    , 'open_acc_6m'
    , 'open_il_6m'
    , 'open_il_12m'
    , 'open_il_24m'
    , 'mths_since_rcnt_il'
    , 'total_bal_il'
    , 'il_util'
    , 'open_rv_12m'
    , 'open_rv_24m'
    , 'max_bal_bc'
    , 'all_util'
    , 'inq_fi'
    , 'total_cu_tl'
    , 'inq_last_12m'
    , 'sub_grade'
]

df_create = df_create.drop(cols_to_drop, axis=1)
df_create.head()

"""#DEFINE THE TARGET VARIABLE / LABELING
---
In this dataset, there are several borrowers who can pay on time, some are late, and some even fail to pay.So in this project I will perform modeling for credit risk prediction and targets.

In this dataset, the `loan_status` variable is a variable that can be used as a target variable because it reflects the performance of each individual in paying for loans/credit thus far.
"""

# Value_counts() to get the counts of each category and then select the top 3
top_3_loan_status = df_create['loan_status'].value_counts().nlargest(3).index

# Filter the DataFrame to include only the top 3 categories
df_top_3 = df_create[df_create['loan_status'].isin(top_3_loan_status)]

# Create the countplot for the top 3 categories
sns.countplot(x='loan_status', data=df_top_3, palette="Set1")
plt.show()

df_create.loan_status.value_counts(normalize=True)*100

"""It can be seen that the `loan_status` variable has several values:
- `Current` means the payment is in date
- `Charged Off` means that the payment is in bad debt so it is written off
- `Late` means the payment was made late
- `In Grace Period` means within the grace period
- `Fully Paid` means payment in full
- `Default` means payment in default

From these definitions, each borrower can be categorized as a good borrower and a bad borrower.

The next stage can be said to be bad borrowers are borrowers who are late in payment until they default
"""

bad_status = [
    'Charged Off'
    , 'Default'
    , 'Does not meet the credit policy. Status:Charged Off'
    , 'Late (31-120 days)'
]

df_create['bad_flag'] = np.where(df_create['loan_status'].isin(bad_status), 1, 0)

df_create['bad_flag'].value_counts(normalize=True)*100

"""After separating between good borrowers and bad borrowers, it can be seen that there is imbalanced data because the good borrower data is 89% while the bad borrower is only 10%."""

df_create.drop('loan_status', axis=1, inplace=True)

"""
#DATA CLEANSING, DATA PREPROCESSING & FEATURE ENGINEERING
----
In this step, some features are cleaned/modified into a format that can be used for modeling.
##emp_length
`emp_length` is each borrower's data on the length of their work period.

This step is to modify `emp_length`. Example: 3 years becomes 3
"""

df_create['emp_length'].unique()

df_create['emp_length_int'] = df_create['emp_length'].str.replace('\+ years', '')
df_create['emp_length_int'] = df_create['emp_length_int'].str.replace('< 1 year', str(0))
df_create['emp_length_int'] = df_create['emp_length_int'].str.replace(' years', '')
df_create['emp_length_int'] = df_create['emp_length_int'].str.replace(' year', '')

df_create['emp_length_int'] = df_create['emp_length_int'].astype(float)

df_create['emp_length_int'].unique()

df_create.drop('emp_length', axis=1, inplace=True)

"""## term
Modify term,`term` here indicates the tenor of the loan

The `term` column will also be simplified from 36 months to just the number 36
"""

df_create['term'].unique()

df_create['term_int'] = df_create['term'].str.replace(' months', '')
df_create['term_int'] = df_create['term_int'].astype(float)

df_create.drop('term', axis=1, inplace=True)

"""## earliest_cr_line

The `earliest_cr_line` column is the date when the credit account was created.

To simplify the next step, the `earliest_cr_line` column will be modified from the month-year format to a calculation of how much time has passed since that time.

Generally, reference date = today is used. However, since this dataset is a 2007-2014 dataset, it would be more relevant to use a reference date around 2017. In this example, 2017-12-01 is used as the reference date.
"""

df_create['earliest_cr_line'].head(3)

df_create['earliest_cr_line_date'] = pd.to_datetime(df_create['earliest_cr_line'], format='%b-%y')
df_create['earliest_cr_line_date'].head(3)

df_create['mths_since_earliest_cr_line'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - df_create['earliest_cr_line_date']) / np.timedelta64(1, 'M')))
df_create['mths_since_earliest_cr_line'].head(3)

df_create['mths_since_earliest_cr_line'].describe()

"""**min      -612.000000**

There is a strange value, which is negative.
"""

df_create[df_create['mths_since_earliest_cr_line']<0][['earliest_cr_line', 'earliest_cr_line_date', 'mths_since_earliest_cr_line']].head(3)

"""Turns out that the negative value came because the Python function misinterpreted the year 62 to be the year 2062, when it should have been the year 1962.

To solve this I only changed the negative value to the maximum value of the feature. Since I know that negative values mean old data (1900s), it makes sense to change those values to the largest value.
"""

df_create.loc[df_create['mths_since_earliest_cr_line']<0, 'mths_since_earliest_cr_line'] = df_create['mths_since_earliest_cr_line'].max()

##df_create[df_create['mths_since_earliest_cr_line']<0][['earliest_cr_line', 'earliest_cr_line_date', 'mths_since_earliest_cr_line']].head(3)

df_create['mths_since_earliest_cr_line'].describe()

df_create.drop(['earliest_cr_line', 'earliest_cr_line_date'], axis=1, inplace=True)

"""## issue_d

The `issue_d` column contains information on the month in which the loan was funded.

The preprocessing concept is similar to the one performed on the `earliest_cr_line` variable.

"""

df_create['issue_d_date'] = pd.to_datetime(df_create['issue_d'], format='%b-%y')
df_create['mths_since_issue_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - df_create['issue_d_date']) / np.timedelta64(1, 'M')))

df_create.mths_since_issue_d.describe()

df_create.drop(['issue_d', 'issue_d_date'], axis=1, inplace=True)

"""## last_pymnt_d

`last_pymnt_d` is the last payment information received.

"""

df_create['last_pymnt_d_date'] = pd.to_datetime(df_create['last_pymnt_d'], format='%b-%y')
df_create['mths_since_last_pymnt_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - df_create['last_pymnt_d_date']) / np.timedelta64(1, 'M')))

df_create['mths_since_last_pymnt_d'].describe()

df_create.drop(['last_pymnt_d', 'last_pymnt_d_date'], axis=1, inplace=True)

"""## next_pymnt_d

next_pymnt_d is an information column about the next payment.
"""

df_create['next_pymnt_d_date'] = pd.to_datetime(df_create['next_pymnt_d'], format='%b-%y')
df_create['mths_since_next_pymnt_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - df_create['next_pymnt_d_date']) / np.timedelta64(1, 'M')))

df_create['mths_since_next_pymnt_d'].describe()

df_create.drop(['next_pymnt_d_date', 'next_pymnt_d'], axis=1, inplace=True)

"""## last_credit_pull_d

`last_credit_pull_d` is a column containing the latest credit check of the borrower.
"""

df_create['last_credit_pull_d_date'] = pd.to_datetime(df_create['last_credit_pull_d'], format='%b-%y')
df_create['mths_since_last_credit_pull_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - df_create['last_credit_pull_d_date']) / np.timedelta64(1, 'M')))

df_create['mths_since_last_credit_pull_d'].describe()

df_create.drop(['last_credit_pull_d_date', 'last_credit_pull_d'], axis=1, inplace=True)

df_create.head()

"""# EXPLORATORY DATA ANALYSIS

## Correlation Check
"""

len(df_create.select_dtypes(include='number').columns)

num = df_create.select_dtypes(include='number').columns
cat = df_create.select_dtypes(include='object').columns

plt.figure(figsize=(25, 15))
n = 5
for i in range(0, len(num)):
    plt.subplot(n, math.ceil(len(num)/n), i+1)
    sns.boxplot(y=df_create[num[i]], color='#31D8F9', orient='v')
    plt.tight_layout()

plt.figure(figsize=(25, 15))
n = 5
for i in range(0, len(num)):
    plt.subplot(n, math.ceil(len(num)/n), i+1)
    sns.violinplot(y=df_create[num[i]], color='#31D8F9',orient='v')
    plt.tight_layout()

sns.countplot(x ='verification_status', data = df_create, palette = "Set1")
plt.show()

sns.countplot(x ='grade', data = df_create, palette = "Set1")
plt.show()

sns.countplot(y ='purpose', data = df_create, palette = "Set1")
plt.show()

sns.countplot(x ='emp_length_int', data = df_create, palette = "Set1")
plt.show()

sns.countplot(x ='bad_flag', data = df_create, palette = "Set1")
plt.show()

plt.figure(figsize=(30,20))
sns.heatmap(df_create.corr(), cmap='Blues', annot=True, fmt='.2f')

"""In this case, if there are pairs of features that have a high correlation, one of them will be taken. The correlation value used as a benchmark for high correlation is uncertain, mostly 0.7 is used."""

corr_matrix = df_create.corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
to_drop_hicorr = [column for column in upper.columns if any(upper[column] > 0.7)]

to_drop_hicorr

"""# Check Categorical Features"""

df_create.select_dtypes(include='object').nunique()

# remove features that have a very high cardinality and features that have only one unique value.
df_create.drop(['emp_title', 'title', 'application_type'], axis=1, inplace=True)

df_create.select_dtypes(include='object').nunique()

df_create.select_dtypes(exclude='object').nunique()

df_create.drop(['policy_code'], axis=1, inplace=True)

for col in df_create.select_dtypes(include='object').columns.tolist():
    print(df_create[col].value_counts(normalize=True)*100)
    print('\n')

# remove pymnt_plan because just has 1 recomendation data
df_create.drop('pymnt_plan', axis=1, inplace=True)

"""# MISSING VALUES
## Missing Value Checking
"""

print('Missing values status:', df_create.isnull().values.any())
nvc = pd.DataFrame(df_create.isnull().sum().sort_values(), columns=['Total Null Values'])
nvc['Percentage'] = (nvc['Total Null Values']/df_create.shape[0])*100
nvc["Data Type"] = [df_create[col].dtype for col in df_create.columns]
nvc.sort_values(by=["Total Null Values", "Percentage"], ascending=False, inplace=True)
nvc["NULL Values"] = df_create[nvc.index].isnull().sum()
nvc

# columns with missing values above 75% are removed
# then the missing values in some columns can be filled with '0'

df_create.drop('mths_since_last_record', axis=1, inplace=True)
df_create.drop('mths_since_last_major_derog', axis=1, inplace=True)

"""## Filling The Missing Values"""

df_create['annual_inc'].fillna(df_create['annual_inc'].mean(), inplace=True)
df_create['mths_since_earliest_cr_line'].fillna(0, inplace=True)
df_create['acc_now_delinq'].fillna(0, inplace=True)
df_create['total_acc'].fillna(0, inplace=True)
df_create['pub_rec'].fillna(0, inplace=True)
df_create['open_acc'].fillna(0, inplace=True)
df_create['inq_last_6mths'].fillna(0, inplace=True)
df_create['delinq_2yrs'].fillna(0, inplace=True)
df_create['collections_12_mths_ex_med'].fillna(0, inplace=True)
df_create['revol_util'].fillna(0, inplace=True)
df_create['emp_length_int'].fillna(0, inplace=True)
df_create['tot_cur_bal'].fillna(0, inplace=True)
df_create['tot_coll_amt'].fillna(0, inplace=True)
df_create['mths_since_last_delinq'].fillna(-1, inplace=True)

df_create.isna().sum()

df_create['mths_since_last_pymnt_d'].fillna(df_create['mths_since_last_pymnt_d'].median(), inplace=True)
df_create['mths_since_next_pymnt_d'].fillna(df_create['mths_since_next_pymnt_d'].median(), inplace=True)
df_create['mths_since_last_credit_pull_d'].fillna(df_create['mths_since_last_credit_pull_d'].median(), inplace=True)

df_create['total_rev_hi_lim'].fillna(df_create['total_rev_hi_lim'].mean(), inplace=True)

df_create.isna().sum()

df_create.sample()

"""# One Hot Encoding"""

categorical_cols = [col for col in df_create.select_dtypes(include='object').columns.tolist()]

onehot = pd.get_dummies(df_create[categorical_cols], drop_first=True)

onehot.head()

"""# Standardization"""

numerical_cols = [col for col in df_create.columns.tolist() if col not in categorical_cols + ['bad_flag']]

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
std = pd.DataFrame(ss.fit_transform(df_create[numerical_cols]), columns=numerical_cols)
std.head()

"""# Transform Dataframe"""

data_model = pd.concat([onehot, std, df_create[['bad_flag']]], axis=1)

"""# MODELING
## Train-Test Split
"""

from sklearn.model_selection import train_test_split
X = data_model.drop('bad_flag', axis=1)
y = data_model['bad_flag']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train.shape, X_test.shape

"""# Train the data"""

from sklearn.linear_model import LogisticRegression #logistic regression
from sklearn.naive_bayes import GaussianNB #gaussian naive bayes
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier #decision tree
from sklearn.ensemble import RandomForestClassifier #random forest
from sklearn.neighbors import KNeighborsClassifier #k-nearest neighbor
from sklearn.svm import SVC # Support Vector Machine/Classifier
from sklearn.neural_network import MLPClassifier #neural network
from sklearn.ensemble import GradientBoostingClassifier #gradient boosting
from xgboost import XGBClassifier #xgboost
from sklearn.ensemble import AdaBoostClassifier #Adaboost

from sklearn import metrics
from sklearn.model_selection import cross_val_score, cross_validate
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
from sklearn.metrics import roc_auc_score, roc_curve #roc score
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.inspection import permutation_importance
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from scipy.stats import uniform

# reimport for backup
import re
import warnings
warnings.filterwarnings('ignore')

"""# Function for Model Evaluation"""

train_classifier_list = []
train_modelname_list = []
train_accuracy_list = []
train_precision_list = []
train_recall_list = []
train_f1_score_list= []
train_roc_auc_score_list = []
train_cross_val_f1_list = []
train_cross_val_rocauc_list = []

test_classifier_list = []
test_modelname_list = []
test_accuracy_list = []
test_precision_list = []
test_recall_list = []
test_f1_score_list= []
test_roc_auc_score_list = []
test_cross_val_f1_list = []
test_cross_val_rocauc_list = []
from sklearn.utils import shuffle
from sklearn.model_selection import KFold
X_s, y_s = shuffle(X, y, random_state=42)
kf = KFold(10, shuffle=True, random_state=0)

"""# Function Evaluation for Recap Classification"""

def eval_classification(model, model_name, save=True):
    # predict train
    y_train_pred = model.predict(X_train)
    y_train_pred_prob = model.predict_proba(X_train)

    # predict test
    y_test_pred = model.predict(X_test)
    y_test_pred_prob = model.predict_proba(X_test)

    # cross validation
    cv_score_f1 = cross_validate(model, X_s, y_s, cv=kf, scoring='f1', return_train_score=True)
    cv_score_rocauc = cross_validate(model, X_s, y_s, cv=kf, scoring='roc_auc', return_train_score=True)


    accuracy_train = round(accuracy_score(y_train, y_train_pred), 3)
    precision_train = round(precision_score(y_train, y_train_pred), 3)
    recall_train = round(recall_score(y_train, y_train_pred), 3)
    f1_s_train = round(f1_score(y_train, y_train_pred), 3)
    csf_score_train = round(cv_score_f1['train_score'].mean(), 3)
    rocauc_score_train = round(roc_auc_score(y_train, y_train_pred_prob[:, 1]), 3)
    csr_score_train = round(cv_score_rocauc['train_score'].mean(), 3)

    accuracy_test = round(accuracy_score(y_test, y_test_pred), 3)
    precision_test = round(precision_score(y_test, y_test_pred), 3)
    recall_test = round(recall_score(y_test, y_test_pred), 3)
    f1_s_test = round(f1_score(y_test, y_test_pred), 3)
    csf_score_test = round(cv_score_f1['test_score'].mean(), 3)
    rocauc_score_test = round(roc_auc_score(y_test, y_test_pred_prob[:, 1]), 3)
    csr_score_test = round(cv_score_rocauc['test_score'].mean(), 3)

    if save :

        # save report detail train
        train_classifier_list.append(model)
        train_modelname_list.append(model_name)
        train_accuracy_list.append(accuracy_train)
        train_precision_list.append(precision_train)
        train_recall_list.append(recall_train)
        train_f1_score_list.append(f1_s_train)
        train_cross_val_f1_list.append(csf_score_train)
        train_roc_auc_score_list.append(rocauc_score_train)
        train_cross_val_rocauc_list.append(csr_score_train)

        # save report detail test
        test_classifier_list.append(model)
        test_modelname_list.append(model_name)
        test_accuracy_list.append(accuracy_test)
        test_precision_list.append(precision_test)
        test_recall_list.append(recall_test)
        test_f1_score_list.append(f1_s_test)
        test_cross_val_f1_list.append(csf_score_test)
        test_roc_auc_score_list.append(rocauc_score_test)
        test_cross_val_rocauc_list.append(csr_score_test)


    metrics_summary = pd.DataFrame({
        'Evaluation Metrics' : ["Accuracy", "Precision", "Recall", "F1 Score", "F1 Score (crossval)", "ROC AUC", "ROC AUC (crossval)"],
        'Train' : [accuracy_train, precision_train, recall_train, f1_s_train, csf_score_train, rocauc_score_train, csr_score_train],
        'Test' : [accuracy_test, precision_test, recall_test, f1_s_test, csf_score_test, rocauc_score_test, csr_score_test]})

    metrics_summary["Diff Range"] = metrics_summary['Train'] - metrics_summary['Test']
    return metrics_summary.reset_index(drop = True).style.background_gradient(cmap='Purples')

# define function to see the best tuning hyperparameter
def show_best_hyperparameter(model):
    print(model.best_estimator_.get_params())

"""# Function Evaluation for Training"""

def model_eval_train(classifier, model_name, X_train, y_train):
    # predict data train
    y_train_pred = classifier.predict(X_train)
    y_train_pred_prob = classifier.predict_proba(X_train)

    # print classification report
    print('Classification Report Training Model ('+model_name+'):\n')
    accuracy = round(accuracy_score(y_train, y_train_pred), 3)
    precision = round(precision_score(y_train, y_train_pred), 3)
    recall = round(recall_score(y_train, y_train_pred), 3)
    f1_s = round(f1_score(y_train, y_train_pred), 3)
    rocauc_score = round(roc_auc_score(y_train, y_train_pred_prob[:, 1]), 3)

    # c_val_score = round(cross_val_score(classifier, X_s, y_s, cv=kf, scoring='roc_auc').mean()  , 3)
    cv_score_f1 = cross_validate(classifier, X_s, y_s, cv=kf, scoring='f1', return_train_score=True)
    csf_score = round(cv_score_f1['train_score'].mean(), 3)

    cv_score_rocauc = cross_validate(classifier, X_s, y_s, cv=kf, scoring='roc_auc', return_train_score=True)
    csr_score = round(cv_score_rocauc['train_score'].mean(), 3)

    print(f'Accuracy = {accuracy}')
    print(f'Precision = {precision}')
    print(f'Recall = {recall}')
    print(f'F1 Score = {f1_s}')
    print(f'Cross Val F1 (k=5) = {csf_score}')
    print(f'ROC AUC = {rocauc_score}')
    print(f'Cross Val ROC AUC (k=5) = {csr_score}\n')

    print(classification_report(y_train, y_train_pred))

    # form confusion matrix as a dataFrame
    conf_matrix = pd.DataFrame((confusion_matrix(y_train, y_train_pred)), ('good', 'bad'), ('good', 'bad'))
    tn, fp, fn, tp = confusion_matrix(y_train, y_train_pred).ravel()

    print("==== Actual Data (Train) =====")
    print("Total =", len(y_train))
    print("good =", len(y_train[y_train == 0]))
    print("bad =", len(y_train[y_train == 1]))
    print("==== Predicted Data (Train) =====")
    print("TP = {}, FP = {}, TN = {}, FN = {}".format(tp, fp, tn, fn))
    print("Predictly Correct =", tn+tp)
    print("Predictly Wrong =", fn+fp, "\n")

    # plot confusion matrix
    plt.figure(figsize=[8,5])

    c_matrix = confusion_matrix(y_train, y_train_pred)
    names = ['True Negative', 'False Positive', 'False Negative', 'True Positive']
    counts = ['{0:0.0f}'.format(value) for value in c_matrix.flatten()]
    percentages = ['{0:.2%}'.format(value) for value in c_matrix.flatten() / np.sum(c_matrix)]
    labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(names, counts, percentages)]
    labels = np.asarray(labels).reshape(2, 2)

    heatmap = sns.heatmap(conf_matrix, annot = labels, annot_kws={'size': 13}, fmt='', cmap='Greens')
    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=13)
    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=0, ha='right', fontsize=13)

    plt.title('Confusion Matrix for Training Model ('+model_name+')\n', fontsize=13, color='black')
    plt.ylabel('Actual Label', fontsize=13)
    plt.xlabel('\nPredicted Label', fontsize=13)
    plt.show()
    print("\n")

    # ROC AUC Curve
    plt.figure(figsize=[8,5])
    fpr, tpr, threshold = roc_curve(y_train, y_train_pred_prob[:, 1])
    plt.plot(fpr, tpr, label = model_name+' (Area (Score) = %0.2f)'%rocauc_score)
    plt.plot([0,1],[0,1],'r--')
    plt.xlim([0.0,1.0])
    plt.ylim([0.0,1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Is Data Good or Bad')
    plt.legend(loc="lower right")
    plt.show()

"""# Function Evaluation for Test"""

def model_eval_test(classifier, model_name, X_test, y_test):

     # predict data test
    y_test_pred = classifier.predict(X_test)
    y_test_pred_prob = classifier.predict_proba(X_test)


    # print classification report
    print('Classification Report Testing Model ('+model_name+'):\n')
    accuracy = round(accuracy_score(y_test, y_test_pred), 3)
    precision = round(precision_score(y_test, y_test_pred), 3)
    recall = round(recall_score(y_test, y_test_pred), 3)
    f1_s = round(f1_score(y_test, y_test_pred), 3)
    rocauc_score = round(roc_auc_score(y_test, y_test_pred_prob[:, 1]), 3)

    # c_val_score = round(cross_val_score(classifier, X_s, y_s , cv=kf , scoring='roc_auc').mean()  , 3)
    cv_score_f1 = cross_validate(classifier, X_s, y_s, cv=kf, scoring='f1', return_train_score=True)
    csf_score = round(cv_score_f1['test_score'].mean(), 3)

    cv_score_rocauc = cross_validate(classifier, X_s, y_s, cv=kf, scoring='roc_auc', return_train_score=True)
    csr_score = round(cv_score_rocauc['test_score'].mean(), 3)

    print(f'Accuracy = {accuracy}')
    print(f'Precision = {precision}')
    print(f'Recall = {recall}')
    print(f'F1 Score = {f1_s}')
    print(f'Cross Val F1 (k=5) = {csf_score}')
    print(f'ROC AUC = {rocauc_score}')
    print(f'Cross Val ROC AUC (k=5) = {csr_score}\n')

    print(classification_report(y_test, y_test_pred))

    # form confusion matrix as a dataFrame
    conf_matrix = pd.DataFrame((confusion_matrix(y_test, y_test_pred)), ('good', 'bad'), ('good', 'bad'))
    tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()

    print("==== Actual Data (Test) =====")
    print("Total =", len(y_test))
    print("good =", len(y_test[y_test == 0]))
    print("bad =", len(y_test[y_test == 1]))
    print("==== Predicted Data (Test) =====")
    print("TP = {}, FP = {}, TN = {}, FN = {}".format(tp, fp, tn, fn))
    print("Predictly Correct =", tn+tp)
    print("Predictly Wrong =", fn+fp, "\n")

    # plot confusion matrix
    plt.figure(figsize=[8,5])

    c_matrix = confusion_matrix(y_test, y_test_pred)
    names = ['True Negative', 'False Positive', 'False Negative', 'True Positive']
    counts = ['{0:0.0f}'.format(value) for value in c_matrix.flatten()]
    percentages = ['{0:.2%}'.format(value) for value in c_matrix.flatten() / np.sum(c_matrix)]
    labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(names, counts, percentages)]
    labels = np.asarray(labels).reshape(2, 2)

    heatmap = sns.heatmap(conf_matrix, annot = labels, annot_kws={'size': 13}, fmt='', cmap='Oranges')
    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=13)
    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=0, ha='right', fontsize=13)

    plt.title('Confusion Matrix for Testing Model ('+model_name+')\n', fontsize=13, color='black')
    plt.ylabel('Actual Label', fontsize=13)
    plt.xlabel('\nPredicted Label', fontsize=13)
    plt.show()
    print("\n")

    # ROC AUC Curve
    plt.figure(figsize=[8,5])
    fpr, tpr, threshold = roc_curve(y_test, y_test_pred_prob[:, 1])
    plt.plot(fpr, tpr, label = model_name+' (Area (Score) = %0.2f)'%rocauc_score)
    plt.plot([0,1],[0,1],'r--')
    plt.xlim([0.0,1.0])
    plt.ylim([0.0,1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Is Data Good or Bad')
    plt.legend(loc="lower right")
    plt.show()

# train the model Decision Tree
dt_model = DecisionTreeClassifier(random_state=42).fit(X_train,y_train)
print(dt_model)
eval_classification(dt_model, "Decision Tree")

model_eval_train(dt_model, "Decision Tree", X_train, y_train)

model_eval_test(dt_model, "Decision Tree", X_test, y_test)

acc_dt_train=round(dt_model.score(X_train,y_train)*100,2)
acc_dt_test=round(dt_model.score(X_test,y_test)*100,2)
print("Training Accuracy: {} %".format(acc_dt_train))
print("Testing Accuracy: {} %".format(acc_dt_test))

# train the model Logistic Regression
log_model = LogisticRegression(solver='lbfgs', max_iter=len(X_train), random_state=42).fit(X_train, y_train)
print(log_model)
eval_classification(log_model, "Logistic Regression")

model_eval_train(log_model, "Logistic Regression", X_train, y_train)

model_eval_test(log_model, "Logistic Regression", X_test, y_test)

acc_log_train=round(log_model.score(X_train,y_train)*100,2)
acc_log_test=round(log_model.score(X_test,y_test)*100,2)
print("Training Accuracy: {} %".format(acc_log_train))
print("Test Accuracy: {} %".format(acc_log_test))

results_eval = pd.DataFrame({
    "Models" : train_modelname_list,
    "Precision (Train)": train_precision_list,
    "Precision (Test)": test_precision_list,
    "Recall (Train)": train_recall_list,
    "Recall (Test)": test_recall_list,
    "F1 Score (Train)" : train_f1_score_list,
    "F1 Score (Test)" : test_f1_score_list
})

results_eval.drop_duplicates(inplace = True)

results_eval.sort_values(by=["F1 Score (Test)", "Precision (Test)", "Recall (Test)"], ascending=[False, False, False]).reset_index(drop = True).style.background_gradient(cmap="Purples")

"""# MODELING
## Train-Test Split
"""

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(max_depth=4)
rfc.fit(X_train, y_train)

arr_feature_importances = rfc.feature_importances_
arr_feature_names = X_train.columns.values

df_feature_importance = pd.DataFrame(index=range(len(arr_feature_importances)), columns=['feature', 'importance'])
df_feature_importance['feature'] = arr_feature_names
df_feature_importance['importance'] = arr_feature_importances
df_all_features = df_feature_importance.sort_values(by='importance', ascending=False)
df_all_features

"""# Validation"""

y_pred_proba = rfc.predict_proba(X_test)[:][:,1]

df_actual_predicted = pd.concat([pd.DataFrame(np.array(y_test), columns=['y_actual']), pd.DataFrame(y_pred_proba, columns=['y_pred_proba'])], axis=1)
df_actual_predicted.index = y_test.index

"""## AUC"""

from sklearn.metrics import roc_curve, roc_auc_score

fpr, tpr, tr = roc_curve(df_actual_predicted['y_actual'], df_actual_predicted['y_pred_proba'])
auc = roc_auc_score(df_actual_predicted['y_actual'], df_actual_predicted['y_pred_proba'])

plt.plot(fpr, tpr, label='AUC = %0.4f' %auc)
plt.plot(fpr, fpr, linestyle = '--', color='k')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()

"""# KS"""

df_actual_predicted = df_actual_predicted.sort_values('y_pred_proba')
df_actual_predicted = df_actual_predicted.reset_index()

df_actual_predicted['Cumulative N Population'] = df_actual_predicted.index + 1
df_actual_predicted['Cumulative N Bad'] = df_actual_predicted['y_actual'].cumsum()
df_actual_predicted['Cumulative N Good'] = df_actual_predicted['Cumulative N Population'] - df_actual_predicted['Cumulative N Bad']
df_actual_predicted['Cumulative Perc Population'] = df_actual_predicted['Cumulative N Population'] / df_actual_predicted.shape[0]
df_actual_predicted['Cumulative Perc Bad'] = df_actual_predicted['Cumulative N Bad'] / df_actual_predicted['y_actual'].sum()
df_actual_predicted['Cumulative Perc Good'] = df_actual_predicted['Cumulative N Good'] / (df_actual_predicted.shape[0] - df_actual_predicted['y_actual'].sum())

df_actual_predicted.head()

KS = max(df_actual_predicted['Cumulative Perc Good'] - df_actual_predicted['Cumulative Perc Bad'])

plt.plot(df_actual_predicted['y_pred_proba'], df_actual_predicted['Cumulative Perc Bad'], color='r')
plt.plot(df_actual_predicted['y_pred_proba'], df_actual_predicted['Cumulative Perc Good'], color='b')
plt.xlabel('Estimated Probability for Being Bad')
plt.ylabel('Cumulative %')
plt.title('Kolmogorov-Smirnov:  %0.4f' %KS)

"""## Model yang dibangun menghasilkan performa AUC = 0.9801 dan KS = 0.9002. Pada dunia credit risk modeling, umumnya AUC di atas 0.7 dan KS di atas 0.3 sudah termasuk performa yang baik."""